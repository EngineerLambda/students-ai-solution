import streamlit as st
from transformers import pipeline
from PIL import Image
import requests

# Initialize the BLIP pipeline
#found out about this st.cache_data
@st.cache_data
def load_model():
    return pipeline("image-to-text", model="Salesforce/blip-image-captioning-base")

captioner = load_model()

#Get the app
st.title("Image Captioning with BLIP")
st.expander("This helps to change images to text, using BLIP as the base model")
st.write("Upload an image and get a caption generated by the BLIP model.")

# File uploader
uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

with st.spinner("Processing"):
    if uploaded_file is not None:
        # Display the uploaded image
        image = Image.open(uploaded_file)
        st.image(image, caption="Uploaded Image", use_column_width=True)

        # Generate caption
        st.write("Generating caption...")
        caption = captioner(image)
        st.write(f"**Caption:** {caption[0]['generated_text']}")
